# Development Memo

## How I Actually Used AI While Building

Throughout the development of GuardRail, I relied heavily on Lovable's AI-powered development platform. I had initially prototyped a basic version in Cursor, but Lovable became my primary tool for the complete rebuild. I experimented with AI Studio early on, but quickly found it was better suited for quick prototyping and exploration rather than actual development work.

The AI handled a wide range of tasks across the project. For code generation, it scaffolded the entire React application structure, built out UI components with Tailwind CSS, and implemented the chatbot widget functionality. On the design side, the AI helped translate visual mockups into working code and created the alternating layout pattern for feature sections. For content, it generated initial placeholder copy that I then refined to match GuardRail's voice and mission.

The most critical human intervention came in three areas. First, getting specific implementation details right required careful review and iterative refinement. The AI could generate functional code quickly, but I had to ensure it matched the exact behavior I wanted, especially for the chat widget's state management and the responsive layouts. Second, the design eye was purely human. While the AI could implement visual concepts, I made all the decisions about spacing, color choices, typography, and overall aesthetic coherence. The AI doesn't have taste, it executes instructions. Third, the product messaging and copywriting needed significant human crafting. The AI could generate educational-sounding text, but the specific language around "turning shortcuts into teaching moments" and the careful positioning of GuardRail's three-pillar approach required deliberate human writing that understood both the education space and the target audience of overworked teachers.

---

## Why the AI Feature Looks the Way It Does

I chose to build a teacher-facing chatbot as the AI feature for a practical reason: it directly addresses a real user need at the moment they're evaluating the product. When teachers land on the GuardRail website, they naturally have questions about how it works, how to implement it, and whether it fits their classroom. A chatbot can answer those questions immediately, which both demonstrates the product's AI capabilities and provides genuine utility in the moment.

The original scope for this project was much more ambitious. I initially planned to build a working MVP of the actual GuardRail Chrome extension that would intercept student AI requests in real-time. But that would have required building browser extension infrastructure, setting up a backend to handle request interception, and creating a database to store interaction patterns. It was too much for the time available and would have shifted focus away from the core goal of having a functional AI component. So I scoped down to focus on enhancing the product's marketing website with a genuinely useful AI feature. The chatbot still demonstrates conversational AI capabilities and gives teachers a hands-on experience with the supportive, educational tone that GuardRail would use throughout the full product.

The connection to GuardRail's core value proposition is strong, even if the chatbot itself isn't the main product. GuardRail's entire mission is about using AI to support learning rather than replace it. The Classroom Coach chatbot embodies that philosophy by helping teachers understand and navigate AI tools in education. It models the same "guide, don't give answers" approach that the full GuardRail product would use with students. When a teacher asks the chatbot a question, it provides contextual guidance and explains concepts rather than just dumping information. This preview of how GuardRail "thinks" helps teachers understand what the product would actually do in their classroom. The chatbot also demonstrates that GuardRail isn't anti-AI, it's about channeling AI toward productive educational outcomes, which is the exact value proposition the full product delivers.

---

## Risks, Trade-offs, and Integrity

For this prototype, privacy and security concerns are minimal because no real data is collected or processed. The chatbot uses pattern matching rather than a live API, and the waitlist form is frontend-only with no backend storage. But these considerations would become critical for the full product. If GuardRail actually intercepted student AI interactions, it would need to be FERPA compliant, implement proper data encryption, establish clear retention policies, and potentially handle parental consent for K-12 contexts. The full product would also need transparent communication about what data gets logged and who can access it.

Bias and fairness are harder to address even in a simple chatbot. The system prompt I wrote for the Classroom Coach was designed to be supportive and non-judgmental, but there's inherent risk whenever AI advises educators. I explicitly avoided prescriptive language about what teachers "should" do, instead offering options and explanations. But even with careful prompt engineering, the chatbot could inadvertently favor certain teaching styles, make assumptions about student populations, or reflect biases in its training data. A production version would need ongoing evaluation and refinement, potentially with input from diverse educators across different school contexts.

The risk of over-reliance on AI tools is something I thought about both for the product and for my own development process. For the product, I deliberately designed the chatbot to emphasize that "GuardRail enhances your expertise, not replaces it." The language throughout the site positions GuardRail as a tool that surfaces insights for teachers to act on, not an automated decision-maker. Teachers should use GuardRail's learning gap analysis as one input into their professional judgment, not as gospel. For my own work on this project, I tried to maintain a similar balance. I used AI extensively for code generation and UI scaffolding, but I made all the strategic decisions about product positioning, feature prioritization, and implementation approach. The AI was a powerful accelerant but never the decision-maker.

Regarding academic integrity in my own work, I've been transparent about the AI's role throughout. The code the AI generates is visible and editable, I can see exactly what it wrote and modify it. I haven't hidden that I used AI heavily for development, this memo explicitly documents it. The distinction I've tried to maintain is using AI for execution (writing code, generating layouts, creating components) while doing the thinking myself (deciding what to build, understanding user needs, defining product strategy, crafting messaging). Using AI to build faster doesn't feel like cheating because I'm still making all the hard decisions about what should exist and why.

The biggest explicit limitation I put on the AI feature was keeping it simple and focused. The chatbot only answers questions about GuardRail and classroom AI strategies. It doesn't try to be a general teaching assistant or provide curriculum advice beyond GuardRail's scope. This constraint exists partly for practical reasons (easier to build, more reliable responses) but also because I don't want to overpromise what GuardRail can do. The chatbot is a demo and a utility, not the core product. Keeping its scope narrow reinforces that this is a preview of GuardRail's supportive AI approach, not the full platform.

---

## What I Learned About Building with GenAI

The biggest surprise was how efficient the AI was and how little prompting it needed to generate a decent UI. I expected to spend hours iterating on component layouts and styling, but Lovable could take a single detailed prompt with visual references and produce something close to what I wanted on the first try. The speed at which I could go from idea to working prototype was genuinely surprising. The challenge was different than I expected: instead of struggling to get the AI to understand what I wanted, the struggle was in knowing exactly what I wanted and being able to articulate it clearly. Vague prompts produced vague results, but specific prompts with clear examples produced surprisingly good work.

If I were advising another founder on using GenAI tools well, I'd emphasize knowing what tools are best at what jobs. Claude is excellent for code generation, debugging, and architectural discussions. But it can't make a compelling slide deck the way Notebook LM could, and it's not great at visual design from scratch. Different AI tools have different strengths. The key is matching the tool to the task rather than forcing one tool to do everything. Understanding these strengths means you can move faster by using the right AI for each part of the project instead of fighting with a tool that's not suited for the job.

This project has changed how I think about AI in my own capstone work and future ventures. I'm more likely to use AI in effective, targeted ways rather than throwing it at every problem I encounter. The experience of building GuardRail showed me that AI is incredibly powerful as an execution partner, it can write code, generate content, and implement designs much faster than I could alone. But it's terrible at making strategic decisions, understanding user needs, or determining what should exist in the first place. For my capstone, that means I'll probably use AI heavily for implementation but rely on human insight for product strategy, user research, and creative direction. The goal is to be fast where AI helps and thoughtful where it doesn't.
